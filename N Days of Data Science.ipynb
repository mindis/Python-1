{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N days of Data Science Studying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective of this notebook is to record down my learning journey of data science. <br><br>\n",
    "I am a data analyst. At work, I did a lot of data manipulation and analysis with R and SAS to get insights to answer the questions the stakeholders are interested in and I had experience in running predictive machine learning models. I knew about the statistical concepts such as r-squared, degree of freedom, chi-squared test, t-tests, however, I don't remember clearly the application and I am not able to explain clearly. I knew the machine learning methods such as liner regression, random forest, SVM and xgboost etc. However, I only ran the models with the packages but I don't really understand many of the concepts including the algorithms, the meaning of the parameters and hyperparameters.I don't know what do many of the codes mean and I am not able to explain clearly the concepts. <br><br>\n",
    "I don't want to be just an analyst who does reports and I shouldn't be an analyst who can't even explain basic concepts to others. I need some concrete learning to improve my skills and my objective is to become a data scientist with NLP specification.<br><br>\n",
    "There are a lot to learn and a lot to acheive. I will try to spare 1 hour daily to study and record.<br><br>\n",
    "I have found some materials to get myself started with and I have started for a couple of weeks before I started this notebook. I plan to record down my learning progress and takeaways daily. Since I didn't do so in the past a few weeks, I am recording it in block now and will do daily log afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1 - Day 7 | Video: Machine Learning with Text in scikit-learn (Done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started with NLP in mind and started to search for some materials. Then I came across the recommendation of Kevin's [PyCon 2016 video](https://www.youtube.com/watch?v=ZiKMIuYidY0). <br><br>As mentioned the by people recommended, it's a great video with Kevin's awesome presentation and explaination of the concepts and details. He summerises the framework of sklearn modelling: import->instantiate->fit->predict. He elaborates the intuitive of text ana;ysis example: 'for this text analysis, sklearn doesn't know what we are analysing. We converted all documents with numeric representation (word count). Thus, any classification model can be used with text problems'. He explains the small details along the way such as why the X and y in sklearn are usually in caps and small letters respectively (matrix vs. vector) and also the usage of Jupyter notebook. <br><br>\n",
    "I spent a few days to watch his video and [typed along the codes](https://github.com/sz3g12/Python/blob/master/Machine%20Learning%20with%20Text%20in%20scikit-learn%20(PyCon%202016).ipynb). It helped me to get started with sklearn, text analysis as well as pandas and Jupyter notebook.<br><br>\n",
    "The example in the video is the prediction of ham and spam messages. Firstly, the data is split into train and test data and the train texts are broken down into bag of words. Then the training data are transformed to document-term-matrix. Afterwards, the training data are fitted to predict the results. we need to split before vectorizing becuase: \n",
    "1.the corpus is too large if we don't split first (i.e. train and test togather) 2. to simulate the real world, the test set won't know all the featues the training sets have. Later, the model evaluation and model comparisions are performed. There are also some fun inspections done, such as what are the words that have high spamminess and what are the spam to ham ratio of each word. Lastly, he covers the basic logic of Naive bayes (conditional probability), which is used in this example and he mentioned about the typical tunning parameters in such analysis. <br><br>\n",
    "I also set up the github repo called [Python](https://github.com/sz3g12/Python) to host the codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 8 - Day 11 | 100-Days-Of-ML-Code (Done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I jumped to the famous github project 100-Days-Of-ML-Code becuase it was featured in Chinese social media as a great ML kickstart project with lots of infograms and handson codes and practices. <br> <br>\n",
    "However, when I really look into the project, I realised that it's more like a record for his own learning. It didn't last 100 days and the infograms and exercises are not every day. For the ones with the exercises, it's more of calling the sklearn packages to apply to the same datasets. <br> <br>\n",
    "Nevertheless, I have spent some days on it. I read all the nice infograms, typed along with the codes and read into what he did during the 50+ days. <br> <br>\n",
    "The great thing about this project is that it led me to some good resources he sourced for such as the [Python Data Science HandBook](https://github.com/jakevdp/PythonDataScienceHandbook/tree/master/notebooks) and [Bloomberg's foundations of machine learning](https://bloomberg.github.io/foml/#lectures). He also shared 3Blue1Brown's channel for great videos about linear algebra, caculus, gradient descent and neural network etc., which I plan to watch later.\n",
    "Also I like his concept of recording his learning journey and sharing the takeaways and resources. That's why I am inspired to do the same now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 12 - Day 20 | Python Data Science HandBook (Done for Pandas part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found this handbook from the 100-Days-Of-ML-Code project and felt that it's a good material to get myself to start with Python for Data Science. Although I have been a R user, I feel that Python is better in the machine learning context now, as the packages are well developed and the codes are more consistent. e.g. For quick modelling, you can find the code chunks from online and apply directly to your models.<br><br>\n",
    "Typed along for a dozen of Jupyter notebooks from Python Data Science Handbook, inlucing basic Numpy and [Pandas foundamentals](https://github.com/sz3g12/Python/tree/master) such as Pandas objects, indexing and slicing, multi-indexing, groupby and handling missing data.\n",
    "This process, although took quick some days and long hours but finally got me become confortable with Python. There were a couple of times that I tried to learn Python but I can never remember the sytaxs: whether should I use loc or iloc, what are the differences between () and []. By following the notebooks step by step and typing along, I understood the different object types in Pandas and Numpy as well as the logic of loc and iloc. I also learnt to find documentations on Jupyter with ? and complete the code with <tab>. <br><br>\n",
    "It also helped me to get familiarised with Jupyter notebook. I didn't know a thing about Jupyter notebook previously and I used Spyder when I tried to learn Python as it's similiar to RStudio. However, after getting to know the bueaty of Jupyter notebook, I really like it! I can use the markdown to make notes and summaries in visually pleasant way and I can include pictures, links and equations in it. During the type-along for these handbooks, I learnt to do the basics Jupyter notebook operations: use markdown, insert image, insert links, set headings and bullet points etc.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 21-23 | Coursera Machine Learning (Week 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to stop coding and start enhancing my theories. I revisited the famous [Andrew Ng Coursera course](https://www.coursera.org/learn/machine-learning) that I have enrolled and dropped for a couple of times. I still remember where I dropped: it's a colorful convex at week 1. I was lost, I didn't know what I was learning and why we jumped to convex at week 1.  <br><br>\n",
    "I started again and I understood the convet is the illustration of a cost function and he is trying to explain the intuition of gradient descent. I realised that he exaplained it quite well after I followed through. I guess there is some calculus basics that I need to brush up though.\n",
    "In the week 1 lecture, he explains the differences and applications of supervised learning and unsupervised learning. He explains the [concepts and intuitions of cost function and gradient descent](https://d3c33hcgiwev3.cloudfront.net/_ec21cea314b2ac7d9e627706501b5baa_Lecture2.pdf?Expires=1590883200&Signature=kgxqexgBnJoKU-HcRm7pv9xjqiwlP2qrx2jt8RRcZbRF0Q1WgYIxOVirDOvIthIOGZgqKzZLpgKBidtmalVIz1UaWMOE4Lsa5Sv8YIBVU4KAXSmxH2KAJsUZ7~XUZvdz99S0XhQyF2vEfu6Iw5ryAj8Z5e2I0kt3WJvPMnUFLSY_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A). I found his explainations very detailed and easy to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 24 | Linear Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Came across a good article by Adarsh Menon about [linear regression using gradient descent](https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931) when brwosing through more articles about it. It has codes to implement the model and it explains the concept well with the 'a person walking down the valley' example, the equations and the simple codes. <br><br>\n",
    "Again, as a way of learning, I typed along [this article in Jupyter with my own notes](http://localhost:8889/notebooks/Desktop/Python/pandas/Linear%20Regression%20using%20Gradient%20Descent.ipynb). During which, I learnt how to insert equations in Jupyter with LaTeX.<br><br>\n",
    "This article also helped me revise the r-squared and adjusted r-squared equations that I should have remembered long ago. <br><br>\n",
    "I took a step further and run the default sklearn linear regression model to compare with the gradient descent model and tried to adjust the parameters to improve the r-squared of the gradient descent algorithm as it was outperfromed by the default sklearn linear regression model. Tried a few rounds of tuning of learning rate and number of iterations to get better r-squred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 25 | Starting this Notebook for Recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watched a couple of linear algebra videos from [3Blue1Brown channel](https://www.youtube.com/watch?v=kYB8IZa5AuE&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=3). They are indeed very good with all the illustrations and animations. Understood about concepts of vector in math, the basis vectors i and j, the vector span and linearly independent vectors. \n",
    "Then I thought of using Jupyter notebook to record down my progress, plans and recources in a more organized way for quick indexing and referencing in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 26 | Coursera Machine Learning (Done with Week 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished week 1 videos and tests for the course. The remaining parts include the gradient descent for linear regression (similar to what I learnt on Day 24) and [linear algera review](https://d3c33hcgiwev3.cloudfront.net/_6e5172607f1af1b6156c070104ca213c_Lecture3.pdf?Expires=1590969600&Signature=kon1dIgiz8MKkBf4XFoGogmh71-pprZ-IkWjZtHbOqJNHdTPwp3145M9Su~uFreakPbMgPTvaX8-tK8aqbkojN3g-g8sM5GJ2~tOunL6ji92s9wsydmP8mF25Di9vc~uhkEgSEIPpdlT2IIKRX8gsugzjpN3ZoUQuG2p0aTFom0_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A). <br><br>\n",
    "Andrew explained the concepts of matrix, vector (n x 1 matrix), matrix addition and multiplication with good illustrations and I learnt about identity matrix and reverse & transpose of matrics. He exaplain some of the concepts with simple math operations, which I think is very helpful.<br><br>\n",
    "With this lecture, I finally have an idea about what's the relationship between data analysis and algerba - e.g.we can [construct matrix multiplication](https://github.com/sz3g12/Python/blob/master/pics/matrix%20matrix%20multiplication.PNG) to calculate the predicted prices of house. It's useful as matrix operations are efficient and we can do all the computation at once. <br><br>\n",
    "I watched abit more of 3Blue1Brown's next videos about linear algebra, trying to enhance my understanding following Andrew's linear algebra review. However, I felt the next few videos are a bit out of the scope I currently need and decided to drop it. Instead, I started with their [first caculus video](). Great opening example and explaination abou the calculation of the are of a circle using calculus!<br><br>\n",
    "I added a portion in my previous gradient descent for linear regression codes - to compare the MSEs of the gradient descent model vs. default sklearn model. As it turned out, the default model still outperforms the gradient descent model by a little bit. <br><br>\n",
    "As Andrew's programming assignemnts for the ML course is based on Octave and Matlab, which I don't intend to learn, I tried to explore whether it's feasible to do the assignments with Python istead. I found that there are already [articles](https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-linear-regression-dd04fba8e137) and [github repos](https://github.com/dibgerge/ml-coursera-python-assignments/tree/master/Exercise1) about the assignments done in Python. Thus, I will do my one in Python also and I can choose the well written codes as my reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 27 | Coursera Machine Learning (Week 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished [Week 2 materials](https://d3c33hcgiwev3.cloudfront.net/_7532aa933df0e5055d163b77102ff2fb_Lecture4.pdf?Expires=1591142400&Signature=GY2YvtnoTgS98JX8N2G09edtiaYvvC9YJZx6MW8nkK32npyFpjpo3KbgEXm9L7dziQ1qxoxCEn9DnBjHJCUohGDvwF0-SrkAruwKqpb~sXojA18~aquzQXZ8IXCe0f~ASHoyZcNbFDov73ID2WdizTxZWRAJJr2HsNoNkDL54xc_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A). Learnt about gradient descent for linear regression with multiple variables. Andrew elaborated on the two ways to speed up the gradient descent - feature scaling (e.g. mean normalization) and adjusting learning rate. He also taught another algorithm for solving the ilnear regression with multiple varaibles - Normal Equation. It solves the result of theta analytically (formula), which does not require adjusting the learning rate nor go on iterations. Andrew did not go into how the calculatition is worked out, so I browsed addtional materials about it. [This video](https://www.youtube.com/watch?v=w3b2DCP78Xs) is a good one that explains the formula in projection point of view. Lastly, he compared gradient descent and normal equation algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Started on the week 2 assignment with Python. Haven't figured out how to submit the assignment. I am referring more to the [medium article](https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-linear-regression-dd04fba8e137) than the github repo I found as many of the codes in that github repo do not work for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 28 | Coursera Machine Learning (Week 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worked on the exercise (basically still typing along). Found [another github repo](https://github.com/andrewenoble/machine-learning-andrew-ng/blob/master/ex1_linear_regression/ex1a_univariate.ipynb) that provides more efficient codes than the medium article. Now referring from both sources. \n",
    "Practised matrix operations (append, transpose, multiplication), function definition (cost function and and gradient descent) and matplotlib in this exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 29 | Standford NLP Course (Lecture 1 and 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jumped to NLP related materials. Chose the Standford NLP course (previously on Coursera but has been taken down), which was recommended in Kevin's github. Found the [slide materials](https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html) and the [course videos](https://www.youtube.com/playlist?list=PLQiyVNMpDLKnZYBTUOlSI9mi9wAErFtFm). <br><br>\n",
    "Watched the videos of Lecture 1 and 2: Introductioin and Basic Text Processing.<br><br>\n",
    "[Lecture 1](http://spark-public.s3.amazonaws.com/nlp/slides/intro.pdf) introduced the applications with NLP and the current progress (e.g. span detection, NER and POS tagging are mostly solved; sentiment analysis, machine translation and information extraction are in good progress, while question answering, paraphrasing, summarization and dialogy are still very hard). What makes NLP hard including ambiguity, non-standard usage, neologism, idioms etc. To tackle these issues, we need more knowledge about language, the world and a way to combine knowledge source. <br><br>\n",
    "[Lecture 2](http://spark-public.s3.amazonaws.com/nlp/slides/textprocessingboth.pdf) introduced the text processing methods, including regular expressions (there is a good tool [RegexPal](https://www.regexpal.com/) to test out the regular expression), word tokenization (issues with tokenization; maximum matching word segmentation algorithm for Chinese language), word normalization (case folding and lemmatization) and stemming (a cruder version of lemmatization; Porter's algorithm is the most common English stemmer). It also introduced how to use decision tree (classification methods) to segment sentences (intuition: e.g. with a lot of blanks after is likely to be EOS | with ? or ! is likly to be EOS. * Final punctuation is period does not necessarily indicates EOS as there are cases such as '0.02', '.etc') <br><br>\n",
    "Finished the last bit of Machile Learning Week 2 exericise (fancy plots with the codes I don't understand)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 30 | NLP and ML (Lecture 3 and Week 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watched videos for NLP [Lecture 3](http://spark-public.s3.amazonaws.com/nlp/slides/med.pdf), minimum edit distance. It's used in spelling check and computation biology. It calculats the least distances from one word/string to another. It can also be used to evaluate machine translation and sppech recognition. The way to do it is to search for the minimum distance from initial state to final stage by using operations such as insertion, deletion and substitution. Then the path cost is calculated.The minimum edit distance can be weighted as some letters are more likely to be mistyped than others (e.g. a and e, m and n: easily being misspelt, j and k, o and p: next to each other on keyboard). There is confusion matrix for spelling errors. Sometimes edit distance is not sufficient and we need to find alighment of the two strings to each other. This can be done by keeping a backtrace. From the backtrace mapping, we can see which letters are matched to get the alignment.<br><br>\n",
    "Watched part 1 of Week 3 ML - [logistic regression](https://d3c33hcgiwev3.cloudfront.net/_964b8d77dc0ee6fd42ac7d8a70c4ffa1_Lecture6.pdf?Expires=1591315200&Signature=hRwpiMeYovPqrQDrxBQFYXNQK8Dibp-7kTIEoSgDoXcead~RgnRwQgye7EYhq7QltZcDDBVywwqTokSGmGeobNwuXaeSAeKesOva6o2Bk48IjnO3grBQgCDu-oTusLMzeIvCioiXPw5ri1-rh0-sV9o0zbR~ZRTVL9uTf2v4tDw_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A). A concept that imterested me is 'decision boundary': for the signoid function h(x) = g(z), when z>0, g(z)>0.5. If we take the threshold of 0.5, there is a conclusion that z>0->y = 1. With this in mind, we are able to draw the linear and non-linear decision boundary for logistic regressions if we know the theta values. Another interesting learning is the cost founction of logistic regression: it captures the intiution that when y = 1 and h(x) = 1, cost = 0, while when y = 1 and h(x)->0, cost -> inf (same for the other way around). We need to keep in mind that y equals to either 0 or 1 in logistic regression. <br><br>\n",
    "Andrew mentioned that the cost function of logistic regression is derived with maximum likelihood method but he didn't go into depth. I googled maximum likelihood and found a video in this channel: [StatQuest](https://www.youtube.com/watch?v=XepXtl9YKwc). I like his explanations and the songs in the begining. I probably will watch more of his other videos when looking for stats concept explnations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 31 | ML Course (Done with Week 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished the week 3 of ML. The final part explained about the overfitting issue and how it can be tackled. In this part, I finally got to know the concepts of [bias and variance and also the regularization](https://d3c33hcgiwev3.cloudfront.net/_7d030d67103ce0e7f39dee1d7f78525c_Lecture7.pdf?Expires=1591401600&Signature=Nha0Iccdg-EO6jbh92n5CKpwmq0m0hEBChts6yaFQH8l1M4WOSU0MgSelu08fzg-dwPcz7KBm0IJoW4fAf8HutIIwk~zwUNWaSqIdE3dgYpnPxsrWiOUAJ4woUuqy~PRidXjN1t1hEPPcbCNf6y6hNOvXjq8wAJ0xXF1HFm9Aus_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A).<br><br>\n",
    "Now I understand the reason that Ng started the lectures with cost function (which scared me away twice). It's indeed the fundamental of the machine learning. The ojective of machine learning is alweay about having better fitting - i.e. minimise the cost function. The concept of regularization is about having reasonably low bias but also avoid over complicated model (because it wwill have high variance). Thus, we introduce penalty to the cost function to achieve this (penelizing large / too many thetas). It works for both gradient descent and normal equation. <br><br>\n",
    "The regularization introduced by Ng is Ridge regression, which adds $\\lambda\\theta^2$ to the cost function. As I remember there are two common ones, L1 and L2, I looked up StatQuest for more details. Again, StatQuest did a great job explaining and there are the videos: [Ridge Regression](https://www.youtube.com/watch?v=Q81RR3yKn30) and [Lasso Regression](https://www.youtube.com/watch?v=NGf0voTMlcs&t=409s). As he summarizes, regularization is essentially 'desensitization. It smooths the curves to give better generalization of training models to be used in test data. He mentioned that Ridge regression can also solve the problem of 'no. of samples < no. of variables' when used together with cross validation. Since he mentioned cross validation, I also watched his video about [cross validation](https://www.youtube.com/watch?v=fSytzGwwBVw). His videos are so good. I can't wait to watch more together with the lecture videos. <br><br>\n",
    ">There are a lot of terms that I have always knew about but never understod. e.g. variance, bias, regularization, learning rate and all. I am happy that I am getting to know them one by one. It's fulfilling. As Andrew said 'You've actually come a long ways. And you can actually, you actually know enough to apply this stuff and get to work for many problems.'. I am glad that I finally get started in going into depth in studying machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 32 | ML Course (Done with Week 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished the Week 4 ML - [neural network introduction](https://d3c33hcgiwev3.cloudfront.net/_48018e8190fedff87b572550690056d2_Lecture8.pdf?Expires=1591574400&Signature=TL7xY2nOORSC~1kdHWQYVLs7dxoDdpC45ISXIOVhJNvx6j8MlLFxR7WkDuNwlfkHEKk5ORb~PuNRAXd-HvDw2GdhdMib84wWt1Nd8U1Aej1bCGC8FlBXsv85uG3C2Db-Pmx9eVcLu2vaWe1bSSyAoIiRQcu0mjPNBzwuWEVmUtE_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A). Ng introduced the model representation and initution of neural network. As clear as he explained, I think I still need to digest for a while and maybe source for other materials too. The idea I grasped is that the nueral network imitates the network of neurons that receive signals (inputs) from other neurons and transmit the signals until the info reaches the output layer (e.g. feedback to brain). So there are input layer, hidden layer and output layer. At each layer, there are connections and weights assigned to connections (THETA). There is also bias unit, which is similar to the $x_0$ in our funcions. The supercript in the $\\alpha_{1}^{(2)}$ indicates the second layer and the 1 inthe subscript means the 1st element in the array. <br><br>\n",
    "The activation function used in the examples is sigmoid function, which give activation value of (0, 1) and we can use it to contruct logical functions. <br><br>\n",
    "After the coursera lecture, I thought that I need to go back to some coding practise and I jumped to Kevin's [video series about sklearn](https://www.youtube.com/playlist?list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A). One thing I like about Kevin, except for this good presentation, is that he always recomend useful resources. I quickly went through his intro videos about [sklearn and Jupyter setup](https://github.com/justmarkham/scikit-learn-videos/blob/master/02_machine_learning_setup.ipynb) and [Iris dataset intro](https://github.com/justmarkham/scikit-learn-videos/blob/master/03_getting_started_with_iris.ipynb). He explained the differences of using [Python/sklearn vs. R for machine learning](https://www.dataschool.io/python-or-r-for-data-science/) and said that sklearn has slightly steeper learning curve in the start and it focuses more on accuracy than explinability as compared to R (which looks at ML from a more stats point of view). Found a good video/deck about '[Fast numerical computing with NumPy](https://speakerdeck.com/jakevdp/losing-your-loops-fast-numerical-computing-with-numpy-pycon-2015)' and went through the deck. Will watcht the [video](https://www.youtube.com/watch?v=EEUXKG97YRw) tomorrow.<br><br>\n",
    "Worth to mention that the video is also from PyCon. Seems like I should watch more good videos from that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 33 | Kevin's sklearn videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today I watched the video of the '[Fast numerical computing with NumPy](https://www.youtube.com/watch?v=EEUXKG97YRw)'. The deck was actually quite self-explainatory. It demostrated how is NumPy faster than Python loops and what are the useful applications for speeding up the computation using NumPy. The four strategies mentioned by the speaker to speed up with NumPy are 1.ufunc 2. aggregation 3. broadcasting 4. slicing, masking and fancy indexing. I have already knew about 1, 2 and 4 from Pandas coding practises; so I watched the video mostly focusing on the NumPy brodcasting application and the KNN example explaination. \n",
    ">The term broadcasting describes how numpy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deepen my understanding, I read this [NumPy article](https://numpy.org/devdocs/user/theory.broadcasting.html) that explains the broadcasting with graphs. Basically, if two metrices can 'match', they can still have arithmetic opreations even if they are in the different sizes. The smaller one can be \"enlarged\" to match the bigger one.The KNN example is a good application for broadcast that can reduce the operations for calculating the Euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I watched a few of Kevin's videos, which gave intro about training and evaluating machine learning models. He summarized the sklearn ML framework, fit a classification model with KNN and logistic regression and explained the upsides and downsides of train/test split method (which can be overcame with cross validation). He recommended an article about [Bias and Variance tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html) with KNN model as example. I am yet to read it. I also come across this mindmap of [sklearn model choice cheatsheet](https://www.oreilly.com/content/wp-content/uploads/sites/2/2019/06/scikit-learn-flow-chart-5d2262f1051e76f9a17e2c6cb451335e.jpg) for future use.\n",
    "<br><br>\n",
    "Lastly, I watch a couple of videos about KNN from my favorite [StatQuest](https://www.youtube.com/watch?v=HVXime0nQeI) and this new amazing channel [Fun and Easy Machine Learning](https://www.youtube.com/watch?v=MDniRwXizWo). So KNN is a method to clasify the samples based on the k nearest neighbors. i.e. if the people closest surrounding you are A, then you are A. k is the no. of the nearest neighbors the models will take to decide which class you belong to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 34 - 35 | Kevin's sklearn videos (Done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished watching kevin's playlist of sklearn videos. Useful takeaways include [cross validation](https://github.com/justmarkham/scikit-learn-videos/blob/master/07_cross_validation.ipynb) and [GridSearch](https://github.com/justmarkham/scikit-learn-videos/blob/master/08_grid_search.ipynb). Finally understood what we have done in the previous project. Also reviewed the [linear regression](https://github.com/justmarkham/DAT5/blob/master/notebooks/09_linear_regression.ipynb) and logistic regression with sklearn as well as [model evaluation for classification model](https://github.com/justmarkham/scikit-learn-videos/blob/master/09_classification_metrics.ipynb) (which contains a lot of info about confusion matrix, auc, precision and recall etc,). Except for sklearn, there is also statsmodels.formula.api for modelling, which provides more summary stats. <br><br>\n",
    "I found this series quite helpful in getting an idea of the framework of applying sklean - from training the model to model tunning, feature selction and model evaluation. More details could be learn from the practical projects. I typed along the codes to have a reference for myself - [Machine learning - Data School](https://github.com/sz3g12/Python/blob/master/Machine%20Learning%20-%20Data%20School.ipynb). I should definitely finish more of his [video series](https://www.dataschool.io/start/) when I have time. <br><br>\n",
    "I also watched Ng's week 5 video about backpropagation. I watched until my eyes are closed..I will try again tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 36 - 37 | Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Started Kaggle titanic. On Day 36, did majority of the data processing in Python without copying the code (write by myself/find info from google). On Day 37, did the first version of LR and got 77%. Encountered issue with trying randon forest (as it turning out, I wrongly used RandomForrestRegressor, not Classifier. \n",
    "I applied pipleline from [Kevin's notebook](https://github.com/justmarkham/scikit-learn-videos/blob/master/10_categorical_features.ipynb) and I found the function make_columns and pipeline very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 38 | Kaggle + Week 6 ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figured out the rf issue and fit the model with default rf and svm model. The training cv results are slightly better than logreg while the test results are wrose off. This might be overfitting issue and need parameter tunning. I should reading more articles from the forums and try understand the tune more parameters as well as trying out other methods such as xgboost, decision tree, neural network.\n",
    "\n",
    "Just when I have no idea about where to go about for tuning the model, I realised that the week 6 ML video is about [Machine learning diagnostic](https://d3c33hcgiwev3.cloudfront.net/_b0cf48c6b7bc9f194310e6bc90dec220_Lecture10.pdf?Expires=1592179200&Signature=GUSDrujjKZnq2ILLSeFWd5VmLPKu9zj~yn8rgwOaglmixzsLcaen2edqWCESANfgE3c9eRsBjoWRxldfqUysvQcSBybtqZrSKyU5GXbuCXJ7DS3EOZEvnjchl5kpTzhTs6erlI1a6KyEUdSTjZuZN4CsyDpfFqiH4e45189RSFI_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)! It's very useful in giving us a few directions in improving the model. \n",
    "1st tip is that, we should always have a validation/cv set, or else the test set accuracy is also biased as we try to minimize the error on that when evaluation and tunning the model\n",
    "2nd is that we can look at the error reduction from the high bias and high variance point of views by comparing the errors of train vs. cv sets / test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 39 | Rest day "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest day due to personal reasons, will try resume tomorrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 40 | Week 6 ML - Machine learning diagnostic\n",
    "\n",
    "First the machine learning diagnostic part and got 100 marks for review questions for the first time. It's very interesting. Firstly, when we want to decide on the $\\lambda$ value, we can plot out the curve for train and cv sets at different $\\lambda$ values. With smaller $\\lambda$, the training error is small and cv error is large. Then the training error will keep increasing as $\\lambda$ reduces the overfit on train. The errors on cv will go down first and then go up, as too high a $\\lambda$ will result in high bias for cv set too.\n",
    "He also talked about how to identify whether the model suffers from high bias or high variance by checking the learning curves: i.e. how will the training and cv errors change after gradually adding more training data into the model. In return, with such info, we are able to debugging the learning algorithm. e.g. if it's high bias, adding more training data will not help. We can try decreasing $\\lambda$ or adding polynomial features. if it's high varaince, we can increase $\\lambda$, adding more sample data or choose smaller sample of features. \n",
    "\n",
    "Found more of Kevin's notebooks about sklearn. e.g. [emsembling](https://github.com/justmarkham/DAT5/blob/master/notebooks/17_ensembling.ipynb) and [bayes](https://github.com/justmarkham/DAT5/blob/master/notebooks/13_bayes_iris.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 41 | Week 6 ML - machine learning work flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part of week 6 ML is also super useful. It's about [how to work on a machine learning model](https://d3c33hcgiwev3.cloudfront.net/_bfa875d182c5d3c11e9a44e72b1e0f49_Lecture11.pdf?Expires=1592265600&Signature=GQzwr~gdO~q1m6Qyj-tvU~WiIMvduZv1gW1wsw3VAggUbMrcilriMcDYQSun81TkOVwOYvngUpu7NQwJN1hINOtTRk3zCgd7pPGOmQyOQm8xl1t-n4pzjASI-vM-vpO76FufSneLQbJC-vRLL8HJP0jBD7Kkyqd9BSSz7IeZN3I_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A). Ng advised that we start off building a quick and dirty model and then plot out the learning curve to have a sense of the direction - more data or more features, regularization? After that, we manually go through the errors from the cv set to check whether there are any systematic trend on errors and what could be potetial feature that can be added to improve the score. Then we can gradually improve the model with different method with a single value metric tp evaluate.\n",
    "\n",
    "Then he talked about precision and recall which I already knew, thanks to the project last time. And I finally remembered the formula of F1 score. \n",
    "\n",
    "He also explained the large data rationale - why and in what circumistances that the model performance would always increase when you add more training data.It works when the features have enough info to predict y accurately (you can know this by asking yourself - whehter an expert would be able to predict the outcome knowing the features) and the model has enough number of features (complex enough). \n",
    "\n",
    "I shall try tunning the titanic model tomorrow. e.g. adding in regulation terms, scale fare var etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N days of Data Science Studying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective of this notebook is to record down my learning journey of data science. <br><br>\n",
    "I am a data analyst. At work, I did a lot of data manipulation and analysis with R and SAS to get insights to answer the questions the stakeholders are interested in and I had experience in running predictive machine learning models. I knew about the statistical concepts such as r-squared, degree of freedom, chi-squared test, t-tests, however, I don't remember clearly the application and I am not able to explain clearly. I knew the machine learning methods such as liner regression, random forest, SVM and xgboost etc. However, I only ran the models with the packages but I don't really understand many of the concepts including the algorithms, the meaning of the parameters and hyperparameters.I don't know what do many of the codes mean and I am not able to explain clearly the concepts. <br><br>\n",
    "I don't want to be just an analyst who does reports and I shouldn't be an analyst who can't even explain basic concepts to others. I need some concrete learning to improve my skills and my objective is to become a data scientist with NLP specification.<br><br>\n",
    "There are a lot to learn and a lot to acheive. I will try to spare 1 hour daily to study and record.<br><br>\n",
    "I have found some materials to get myself started with and I have started for a couple of weeks before I started this notebook. I plan to record down my learning progress and takeaways daily. Since I didn't do so in the past a few weeks, I am recording it in block now and will do daily log afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1 - Day 7 | Video: Machine Learning with Text in scikit-learn (Done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started with NLP in mind and started to search for some materials. Then I came across the recommendation of Kevin's [PyCon 2016 video](https://www.youtube.com/watch?v=ZiKMIuYidY0). <br><br>As mentioned the by people recommended, it's a great video with Kevin's awesome presentation and explaination of the concepts and details. He summerises the framework of sklearn modelling: import->instantiate->fit->predict. He elaborates the intuitive of text ana;ysis example: 'for this text analysis, sklearn doesn't know what we are analysing. We converted all documents with numeric representation (word count). Thus, any classification model can be used with text problems'. He explains the small details along the way such as why the X and y in sklearn are usually in caps and small letters respectively (matrix vs. vector) and also the usage of Jupyter notebook. <br><br>\n",
    "I spent a few days to watch his video and [typed along the codes](https://github.com/sz3g12/Python/blob/master/Machine%20Learning%20with%20Text%20in%20scikit-learn%20(PyCon%202016).ipynb). It helped me to get started with sklearn, text analysis as well as pandas and Jupyter notebook.<br><br>\n",
    "The example in the video is the prediction of ham and spam messages. Firstly, the data is split into train and test data and the train texts are broken down into bag of words. Then the training data are transformed to document-term-matrix. Afterwards, the training data are fitted to predict the results. we need to split before vectorizing becuase: \n",
    "1.the corpus is too large if we don't split first (i.e. train and test togather) 2. to simulate the real world, the test set won't know all the featues the training sets have. Later, the model evaluation and model comparisions are performed. There are also some fun inspections done, such as what are the words that have high spamminess and what are the spam to ham ratio of each word. Lastly, he covers the basic logic of Naive bayes (conditional probability), which is used in this example and he mentioned about the typical tunning parameters in such analysis. <br><br>\n",
    "I also set up the github repo called [Python](https://github.com/sz3g12/Python) to host the codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 8 - Day 11 | 100-Days-Of-ML-Code (Done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I jumped to the famous github project 100-Days-Of-ML-Code becuase it was featured in Chinese social media as a great ML kickstart project with lots of infograms and handson codes and practices. <br> <br>\n",
    "However, when I really look into the project, I realised that it's more like a record for his own learning. It didn't last 100 days and the infograms and exercises are not every day. For the ones with the exercises, it's more of calling the sklearn packages to apply to the same datasets. <br> <br>\n",
    "Nevertheless, I have spent some days on it. I read all the nice infograms, typed along with the codes and read into what he did during the 50+ days. <br> <br>\n",
    "The great thing about this project is that it led me to some good resources he sourced for such as the [Python Data Science HandBook](https://github.com/jakevdp/PythonDataScienceHandbook/tree/master/notebooks) and [Bloomberg's foundations of machine learning](https://bloomberg.github.io/foml/#lectures). He also shared 3Blue1Brown's channel for great videos about linear algebra, caculus, gradient descent and neural network etc., which I plan to watch later.\n",
    "Also I like his concept of recording his learning journey and sharing the takeaways and resources. That's why I am inspired to do the same now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 12 - Day 20 | Python Data Science HandBook (Done for Pandas part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found this handbook from the 100-Days-Of-ML-Code project and felt that it's a good material to get myself to start with Python for Data Science. Although I have been a R user, I feel that Python is better in the machine learning context now, as the packages are well developed and the codes are more consistent. e.g. For quick modelling, you can find the code chunks from online and apply directly to your models.<br><br>\n",
    "Typed along for a dozen of Jupyter notebooks from Python Data Science Handbook, inlucing basic Numpy and [Pandas foundamentals](https://github.com/sz3g12/Python/tree/master) such as Pandas objects, indexing and slicing, multi-indexing, groupby and handling missing data.\n",
    "This process, although took quick some days and long hours but finally got me become confortable with Python. There were a couple of times that I tried to learn Python but I can never remember the sytaxs: whether should I use loc or iloc, what are the differences between () and []. By following the notebooks step by step and typing along, I understood the different object types in Pandas and Numpy as well as the logic of loc and iloc. I also learnt to find documentations on Jupyter with ? and complete the code with <tab>. <br><br>\n",
    "It also helped me to get familiarised with Jupyter notebook. I didn't know a thing about Jupyter notebook previously and I used Spyder when I tried to learn Python as it's similiar to RStudio. However, after getting to know the bueaty of Jupyter notebook, I really like it! I can use the markdown to make notes and summaries in visually pleasant way and I can include pictures, links and equations in it. During the type-along for these handbooks, I learnt to do the basics Jupyter notebook operations: use markdown, insert image, insert links, set headings and bullet points etc.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 21-23 | Coursera Machine Learning (Week 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to stop coding and start enhancing my theories. I revisited the famous [Andrew Ng Coursera course](https://www.coursera.org/learn/machine-learning) that I have enrolled and dropped for a couple of times. I still remember where I dropped: it's a colorful convex at week 1. I was lost, I didn't know what I was learning and why we jumped to convex at week 1.  <br><br>\n",
    "I started again and I understood the convet is the illustration of a cost function and he is trying to explain the intuition of gradient descent. I realised that he exaplained it quite well after I followed through. I guess there is some calculus basics that I need to brush up though.\n",
    "In the week 1 lecture, he explains the differences and applications of supervised learning and unsupervised learning. He explains the [concepts and intuitions of cost function and gradient descent](https://d3c33hcgiwev3.cloudfront.net/_ec21cea314b2ac7d9e627706501b5baa_Lecture2.pdf?Expires=1590883200&Signature=kgxqexgBnJoKU-HcRm7pv9xjqiwlP2qrx2jt8RRcZbRF0Q1WgYIxOVirDOvIthIOGZgqKzZLpgKBidtmalVIz1UaWMOE4Lsa5Sv8YIBVU4KAXSmxH2KAJsUZ7~XUZvdz99S0XhQyF2vEfu6Iw5ryAj8Z5e2I0kt3WJvPMnUFLSY_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A). I found his explainations very detailed and easy to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 24 | Linear Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Came across a good article by Adarsh Menon about [linear regression using gradient descent](https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931) when brwosing through more articles about it. It has codes to implement the model and it explains the concept well with the 'a person walking down the valley' example, the equations and the simple codes. <br><br>\n",
    "Again, as a way of learning, I typed along [this article in Jupyter with my own notes](http://localhost:8889/notebooks/Desktop/Python/pandas/Linear%20Regression%20using%20Gradient%20Descent.ipynb). During which, I learnt how to insert equations in Jupyter with LaTeX.<br><br>\n",
    "This article also helped me revise the r-squared and adjusted r-squared equations that I should have remembered long ago. <br><br>\n",
    "I took a step further and run the default sklearn linear regression model to compare with the gradient descent model and tried to adjust the parameters to improve the r-squared of the gradient descent algorithm as it was outperfromed by the default sklearn linear regression model. Tried a few rounds of tuning of learning rate and number of iterations to get better r-squred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 25 | Starting this Notebook for Recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watched a couple of linear algebra videos from [3Blue1Brown channel](https://www.youtube.com/watch?v=kYB8IZa5AuE&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=3). They are indeed very good with all the illustrations and animations. Understood about concepts of vector in math, the basis vectors i and j, the vector span and linearly independent vectors. \n",
    "Then I thought of using Jupyter notebook to record down my progress, plans and recources in a more organized way for quick indexing and referencing in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 26 | Coursera Machine Learning (Done for Week 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished week 1 videos and tests for the course. The remaining parts include the gradient descent for linear regression (similar to what I learnt on Day 24) and [linear algera review](https://d3c33hcgiwev3.cloudfront.net/_6e5172607f1af1b6156c070104ca213c_Lecture3.pdf?Expires=1590969600&Signature=kon1dIgiz8MKkBf4XFoGogmh71-pprZ-IkWjZtHbOqJNHdTPwp3145M9Su~uFreakPbMgPTvaX8-tK8aqbkojN3g-g8sM5GJ2~tOunL6ji92s9wsydmP8mF25Di9vc~uhkEgSEIPpdlT2IIKRX8gsugzjpN3ZoUQuG2p0aTFom0_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A). <br><br>\n",
    "Andrew explained the concepts of matrix, vector (n x 1 matrix), matrix addition and multiplication with good illustrations and I learnt about identity matrix and reverse & transpose of matrics. He exaplain some of the concepts with simple math operations, which I think is very helpful.<br><br>\n",
    "With this lecture, I finally have an idea about what's the relationship between data analysis and algerba - e.g.we can [construct matrix multiplication](https://github.com/sz3g12/Python/blob/master/pics/matrix%20matrix%20multiplication.PNG) to calculate the predicted prices of house. It's useful as matrix operations are efficient and we can do all the computation at once. <br><br>\n",
    "I watched abit more of 3Blue1Brown's next videos about linear algebra, trying to enhance my understanding following Andrew's linear algebra review. However, I felt the next few videos are a bit out of the scope I currently need and decided to drop it. Instead, I started with their [first caculus video](). Great opening example and explaination abou the calculation of the are of a circle using calculus!<br><br>\n",
    "I added a portion in my previous gradient descent for linear regression codes - to compare the MSEs of the gradient descent model vs. default sklearn model. As it turned out, the default model still outperforms the gradient descent model by a little bit. <br><br>\n",
    "As Andrew's programming assignemnts for the ML course is based on Octave and Matlab, which I don't intend to learn, I tried to explore whether it's feasible to do the assignments with Python istead. I found that there are already [articles](https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-linear-regression-dd04fba8e137) and [github repos](https://github.com/dibgerge/ml-coursera-python-assignments/tree/master/Exercise1) about the assignments done in Python. Thus, I will do my one in Python also and I can choose the well written codes as my reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 27 | Coursera Machine Learning (Week 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished [Week 2 materials](https://d3c33hcgiwev3.cloudfront.net/_7532aa933df0e5055d163b77102ff2fb_Lecture4.pdf?Expires=1591142400&Signature=GY2YvtnoTgS98JX8N2G09edtiaYvvC9YJZx6MW8nkK32npyFpjpo3KbgEXm9L7dziQ1qxoxCEn9DnBjHJCUohGDvwF0-SrkAruwKqpb~sXojA18~aquzQXZ8IXCe0f~ASHoyZcNbFDov73ID2WdizTxZWRAJJr2HsNoNkDL54xc_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A). Learnt about gradient descent for linear regression with multiple variables. Andrew elaborated on the two ways to speed up the gradient descent - feature scaling (e.g. mean normalization) and adjusting learning rate. He also taught another algorithm for solving the ilnear regression with multiple varaibles - Normal Equation. It solves the result of theta analytically (formula), which does not require adjusting the learning rate nor go on iterations. Andrew did not go into how the calculatition is worked out, so I browsed addtional materials about it. [This video](https://www.youtube.com/watch?v=w3b2DCP78Xs) is a good one that explains the formula in projection point of view. Lastly, he compared gradient descent and normal equation algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Started on the week 2 assignment with Python. Haven't figured out how to submit the assignment. I am referring more to the [medium article](https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-linear-regression-dd04fba8e137) than the github repo I found as many of the codes in that github repo do not work for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 28 | Coursera Machine Learning (Week 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worked on the exercise (basically still typing along). Found [another github repo](https://github.com/andrewenoble/machine-learning-andrew-ng/blob/master/ex1_linear_regression/ex1a_univariate.ipynb) that provides more efficient codes than the medium article. Now referring from both sources. \n",
    "Practised matrix operations (append, transpose, multiplication), function definition (cost function and and gradient descent) and matplotlib in this exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 29 | Standford NLP Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jumped to NLP related materials. Chose the Standford NLP course (previously on Coursera but has been taken down), which was recommended in Kevin's github. Found the [slide materials](https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html) and the [course videos](https://www.youtube.com/playlist?list=PLQiyVNMpDLKnZYBTUOlSI9mi9wAErFtFm). <br><br>\n",
    "Watched the videos of Lecture 1 and 2: Introductioin and Basic Text Processing.<br><br>\n",
    "[Lecture 1](http://spark-public.s3.amazonaws.com/nlp/slides/intro.pdf) introduced the applications with NLP and the current progress (e.g. span detection, NER and POS tagging are mostly solved; sentiment analysis, machine translation and information extraction are in good progress, while question answering, paraphrasing, summarization and dialogy are still very hard). What makes NLP hard including ambiguity, non-standard usage, neologism, idioms etc. To tackle these issues, we need more knowledge about language, the world and a way to combine knowledge source. <br><br>\n",
    "[Lecture 2](http://spark-public.s3.amazonaws.com/nlp/slides/textprocessingboth.pdf) introduced the text processing methods, including regular expressions (there is a good tool [RegexPal](https://www.regexpal.com/) to test out the regular expression), word tokenization (issues with tokenization; maximum matching word segmentation algorithm for Chinese language), word normalization (case folding and lemmatization) and stemming (a cruder version of lemmatization; Porter's algorithm is the most common English stemmer). It also introduced how to use decision tree (classification methods) to segment sentences (intuition: e.g. with a lot of blanks after is likely to be EOS | with ? or ! is likly to be EOS. * Final punctuation is period does not necessarily indicates EOS as there are cases such as '0.02', '.etc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
